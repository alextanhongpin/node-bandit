# Multi-armed bandit

You a given a slot machine with multiple arms - each of them will return different rewards. You only have a fixed budget of $100, how do you maximize your rewards in the shortest time possible?

In short, multi-armed bandit:
- is part of probability theory
- is a solution for exploit-explore conundrum
- is a type of reinforcement learning
- maximize rewards in the fastest way

![Bandit](/assets/bandit.png)

## References 

1. https://github.com/iosband/iosband.github.io/blob/master/js/ber_bandits.js
2. https://github.com/curiousily/percipio/blob/master/src/stats.js
3. https://github.com/omphalos/bayesian-bandit.js/blob/master/bayesian-bandit.js
4. https://arxiv.org/pdf/1510.00757.pdf